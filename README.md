# HyperRAG v3 - Geometric RAG Optimization

## Project Goal
Investigate whether alternative geometric structures (beyond the standard hypersphere with cosine similarity) can improve RAG retrieval accuracy, specifically targeting "Semantic Collapse" in large document sets.

## Key Results (Summary)

> **ðŸš€ Hyper-Scale Breakthrough**: Hybrid Radial encoding with **$\alpha=0.05$** successfully mitigates "Semantic Collapse" at scale, outperforming standard RAG by **+0.0031 MRR** when documents increase to 15,000. The improvement grows as the corpus size increases.

### Core Benchmarks (on SQuAD dataset)

| Corpus Scale | Best Approach | Alpha | Gain (MRR) | Status |
| :--- | :--- | :--- | :--- | :--- |
| **15,000 Docs** | **Hybrid Radial** | **0.05** | **+0.31% (abs)** | **Verified** |
| 1,000 Docs | Hybrid Radial | 0.70 | +4.90% (rel) | Verified |
| Any Scale | CrossPolytope L1 | - | +3.10% (rel) | Robust |

### Large Model Results (1024 dims - e5-large-v2)

| Engine | MRR | H@5 | Notes |
|--------|-----|-----|-------|
| **ST Baseline (e5-large)** | **0.7923** | **94.6%** | Best absolute performance |
| Hybrid Radial (a=0.1) | 0.7918 | 94.5% | Statistically tied |
| Whitened RAG (ZCA) | 0.7134 | 86.5% | **-10%** (Failed) |
| ClusterTree RAG | 0.6930 | 82.1% | **-12.5%** (Failed) |

**Key insight**: On highly optimized large models (e5-large), standard cosine similarity is extremely hard to beat. The embedding space is already "perfectly" shaped for cosine. Geometric interventions like **Whitening** or **Clustering** destroy this learned structure.

## Semantic Collapse & Solutions

**"Semantic Collapse"** is the phenomenon where RAG precision drops as the corpus grows. Our 15k-document study confirmed:
1.  **Scaling Gain**: With `e5-large-v2`, the improvement of **Hybrid Radial** over the baseline grows with the corpus (**+0.0031 MRR** at 15k docs vs +0.0024 at 1k).
2.  **Dimension Threshold**: Radial encoding requires models with **>=768 dimensions**. In tiny models (128D - bert-tiny), it introduces noise (-5% MRR).
3.  **Alpha Scaling**: The $\alpha$ parameter must scale inversely with corpus size (e.g., $\alpha=0.05$ for large scale).

**What DOESN'T Work (The "Negative Results" Hall of Shame):**
- **Whitening (ZCA)**: -10% MRR.
- **Clustering**: -12.5% MRR.
- **Hellinger**: -19% MRR.
- **Applying to <512D models**: -1.6% to -5% MRR.

## Recommended Configurations

### 1. Maximum Performance (Unlimited Resources)
Use **`intfloat/e5-large-v2`** with Standard Cosine Similarity.
```python
rag = StandardRAG(model_name='intfloat/e5-large-v2')
```

### 2. High Performance (Constrained/Standard)
Use **`HybridGeometricRAG`** with `all-mpnet-base-v2`.
```python
# Use small alpha (0.1 - 0.2) for larger corpora
rag = HybridGeometricRAG(model_name='all-mpnet-base-v2', use_radial_encoding=True, alpha=0.2)
```

### 3. Efficiency / Sparse Data
Use **`CrossPolytopeRAG`** (L1 distance).
```python
rag = CrossPolytopeRAG(volumetric=True)
```

## Architecture

```
HyperRAG/
â”œâ”€â”€ README.md               # Quick start and results
â”œâ”€â”€ docs/                   # Detailed studies and theory
â”‚   â”œâ”€â”€ RESEARCH.md         # Semantic Collapse & Prior Art
â”‚   â”œâ”€â”€ techniques_explained.md # Geometric theory deep-dive
â”‚   â””â”€â”€ large_model_analysis.md
â”œâ”€â”€ src/                    # Source code
â”‚   â””â”€â”€ HyperRAG/           # Main package
â”‚       â”œâ”€â”€ core/           # Geometry and Bases
â”‚       â”œâ”€â”€ advanced/       # Hybrid and Radial strategies
â”‚       â””â”€â”€ experimental/    # Research trials (Whitening, etc.)
â”œâ”€â”€ benchmarks/             # Study scripts
â”‚   â”œâ”€â”€ hyperscale_benchmark.py # Semantic Collapse study
â”‚   â””â”€â”€ ...
â”œâ”€â”€ results/                # Collected data
â”‚   â”œâ”€â”€ logs/               # Raw execution outputs
â”‚   â”œâ”€â”€ reports/            # JSON summaries
â”‚   â””â”€â”€ plots/              # Visualizations
â””â”€â”€ scripts/                # Helper tools
```

## Future Work

1.  **Learned Radial Projections**: Instead of using heuristics (word count), train a small MLP to predict the optimal "radius" for a document based on its content.
2.  **Query-dependent geometry**: Switch metrics based on whether the query appears to be "lookup" (L1) or "thematic" (Cosine).
3.  **Test on 100k+ Documents**: Validate if "Semantic Collapse" is truly mitigated at massive scale.

## References

- Isoperimetric inequality: Sphere maximizes V/S ratio
- Cross-polytope: Dual of hypercube, L1 unit ball
- PoincarÃ© ball: Hyperbolic geometry for hierarchical data
- **Semantic Collapse**: Degradation of retrieval at scale due to embedding crowding.
